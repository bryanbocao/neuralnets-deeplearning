{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  2.31800003e+01,   2.72719994e+01,   4.26000000e+02,\n",
      "          7.21250000e+02,   4.79298830e-03],\n",
      "       [  2.31499996e+01,   2.72674999e+01,   4.29500000e+02,\n",
      "          7.14000000e+02,   4.78344085e-03],\n",
      "       [  2.31499996e+01,   2.72450008e+01,   4.26000000e+02,\n",
      "          7.13500000e+02,   4.77946363e-03],\n",
      "       ..., \n",
      "       [  2.11000004e+01,   3.60950012e+01,   4.33000000e+02,\n",
      "          7.98500000e+02,   5.59563888e-03],\n",
      "       [  2.11000004e+01,   3.62599983e+01,   4.33000000e+02,\n",
      "          8.20333313e+02,   5.62144956e-03],\n",
      "       [  2.11000004e+01,   3.62000008e+01,   4.47000000e+02,\n",
      "          8.21000000e+02,   5.61206369e-03]], dtype=float32), array([ 1.,  1.,  1., ...,  1.,  1.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading data\n",
    "all_output_0_train_accuracy = 0.787670391748\n",
    "all_output_0_test_accuracy = 0.789889253486\n",
    "all_output_1_train_accuracy = 0.212329608252\n",
    "all_output_1_test_accuracy = 0.210110746514\n",
    "\n",
    "train_data = np.genfromtxt('train_data.txt', delimiter=',')\n",
    "#print \"train_data:\", train_data\n",
    "\n",
    "t = train_data[:,2]\n",
    "hu = train_data[:,3]\n",
    "lt = train_data[:,4]\n",
    "co2 = train_data[:,5]\n",
    "hu_r = train_data[:,6]\n",
    "\n",
    "o = train_data[:,7]\n",
    "\n",
    "#data\n",
    "data = np.column_stack((t, hu, lt, co2, hu_r))\n",
    "data = np.float32(data)\n",
    "#print (\"data:\", data)\n",
    "\n",
    "#print \"np.shape(train_data): \", np.shape(train_data)\n",
    "#print \"np.shape(t): \", np.shape(t)\n",
    "'''\n",
    "print (\"t: \", t)\n",
    "print (\"hu: \", hu)\n",
    "print (\"lt: \", lt)\n",
    "print (\"co2: \", co2)\n",
    "print (\"hu_r: \", hu_r)\n",
    "print (\"o: \", o)\n",
    "'''\n",
    "x_data = np.array(data)\n",
    "y_data = np.array(o)\n",
    "print ([x_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_data: [[ 1.          0.47053295  0.2754904   0.190933    0.55731839]\n",
      " [ 0.99282283  0.47033185  0.27775383  0.18644592  0.55480719]\n",
      " [ 0.99282283  0.4693262   0.2754904   0.18613647  0.55376107]\n",
      " ..., \n",
      " [ 0.50239241  0.86490113  0.28001723  0.23874362  0.76843745]\n",
      " [ 0.50239241  0.87227613  0.28001723  0.25225642  0.77522635]\n",
      " [ 0.50239241  0.8695944   0.2890709   0.25266904  0.77275759]]\n",
      "[array([[ 1.        ,  0.47053295,  0.2754904 ,  0.190933  ,  0.55731839],\n",
      "       [ 0.99282283,  0.47033185,  0.27775383,  0.18644592,  0.55480719],\n",
      "       [ 0.99282283,  0.4693262 ,  0.2754904 ,  0.18613647,  0.55376107],\n",
      "       ..., \n",
      "       [ 0.50239241,  0.86490113,  0.28001723,  0.23874362,  0.76843745],\n",
      "       [ 0.50239241,  0.87227613,  0.28001723,  0.25225642,  0.77522635],\n",
      "       [ 0.50239241,  0.8695944 ,  0.2890709 ,  0.25266904,  0.77275759]], dtype=float32), array([ 1.,  1.,  1., ...,  1.,  1.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    #normalize train data\n",
    "    #normalize t\n",
    "    t = data[:, 0]\n",
    "    t_min = np.min(t)\n",
    "    t_max = np.max(t)\n",
    "    d_t = t_max - t_min\n",
    "    #normalized t: n_t\n",
    "    n_t = []\n",
    "    for each in t:\n",
    "        n_t.append((each - t_min) / d_t)\n",
    "\n",
    "    #normalize hu\n",
    "    hu = data[:, 1]\n",
    "    hu_min = np.min(hu)\n",
    "    hu_max = np.max(hu)\n",
    "    d_hu = hu_max - hu_min\n",
    "    #normalized h: n_hu\n",
    "    n_hu = []\n",
    "    for each in hu:\n",
    "        n_hu.append((each - hu_min) / d_hu)\n",
    "\n",
    "    #normalize lt\n",
    "    lt = data[:, 2]\n",
    "    lt_min = np.min(lt)\n",
    "    lt_max = np.max(lt)\n",
    "    d_lt = lt_max - lt_min\n",
    "    #normalized lt: n_lt\n",
    "    n_lt = []\n",
    "    for each in lt:\n",
    "        n_lt.append((each - lt_min) / d_lt)\n",
    "\n",
    "    #normalize co2\n",
    "    co2 = data[:, 3]\n",
    "    co2_min = np.min(co2)\n",
    "    co2_max = np.max(co2)\n",
    "    d_co2 = co2_max - co2_min\n",
    "    #normalized co2: n_co2\n",
    "    n_co2 = []\n",
    "    for each in co2:\n",
    "        n_co2.append((each - co2_min) / d_co2)\n",
    "\n",
    "    #normalize hu_r\n",
    "    hu_r = data[:, 4]\n",
    "    hu_r_min = np.min(hu_r)\n",
    "    hu_r_max = np.max(hu_r)\n",
    "    d_hu_r = hu_r_max - hu_r_min\n",
    "    #normalized hu_r: n_hu_r\n",
    "    n_hu_r = []\n",
    "    for each in hu_r:\n",
    "        n_hu_r.append((each - hu_r_min) / d_hu_r)\n",
    "\n",
    "    #normalized data: n_data\n",
    "    n_data = np.column_stack((n_t, n_hu, n_lt, n_co2, n_hu_r))\n",
    "\n",
    "    return n_data\n",
    "\n",
    "n_data = normalize(data)\n",
    "print (\"n_data:\", n_data)\n",
    "x_data = np.array(n_data)\n",
    "y_data = np.array(o)\n",
    "print ([x_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test_data: [[ 0.46216769  0.52556469  0.27661817  0.34237253  0.70022289]\n",
      " [ 0.46830266  0.51800397  0.27661817  0.32373573  0.69523877]\n",
      " [ 0.46370143  0.52495038  0.2745098   0.32609151  0.70044674]\n",
      " ..., \n",
      " [ 0.28425358  0.33342784  0.26786844  0.65134541  0.38577152]\n",
      " [ 0.28425358  0.3491636   0.26486401  0.72076222  0.40288059]\n",
      " [ 0.30674847  0.35355826  0.25869703  0.86650613  0.41941993]]\n",
      "o_test: [ 1.  1.  1. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "def get_normalized_test_data(test_data):\n",
    "    test_t = test_data[:,2]\n",
    "    test_hu = test_data[:,3]\n",
    "    test_lt = test_data[:,4]\n",
    "    test_co2 = test_data[:,5]\n",
    "    test_hu_r = test_data[:,6]\n",
    "\n",
    "    new_test_data = np.column_stack((test_t, test_hu, test_lt, test_co2, test_hu_r))\n",
    "    # print \"new_test_data:\", new_test_data\n",
    "\n",
    "    #normalize test data\n",
    "    return normalize(new_test_data)\n",
    "test_data = np.genfromtxt('test_data.txt', delimiter=',')\n",
    "o_test = test_data[:, 7]\n",
    "n_test_data = get_normalized_test_data(test_data)\n",
    "print (\"n_test_data:\", n_test_data)\n",
    "print (\"o_test:\", o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data_batch:  Tensor(\"batch:0\", shape=(100, 8143, 5), dtype=float32)\n",
      "y_data_batch:  Tensor(\"batch_1:0\", shape=(100, 8143), dtype=float64)\n",
      "x_data_batch[0]:  Tensor(\"strided_slice:0\", shape=(8143, 5), dtype=float32)\n",
      "tf.cast(x_data_batch[0][0][0], 'float'):  Tensor(\"strided_slice_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n_input = 5\n",
    "H0 = 10\n",
    "H1 = 5\n",
    "n_output = 1\n",
    "output_num_nodes = 1\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "bs = 100\n",
    "data_size = len(x_data)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "ws0 = tf.Variable(tf.random_uniform([n_input, H0], -1.0, 1.0))\n",
    "ws1 = tf.Variable(tf.random_uniform([H0, H1], -1.0, 1.0))\n",
    "ws2 = tf.Variable(tf.random_uniform([H1, n_output], -1.0, 1.0))\n",
    "\n",
    "H0_outputs = tf.sigmoid(tf.matmul(X, ws0))\n",
    "#H0_outputs = tf.matmul(X, ws0)\n",
    "H1_outputs = tf.sigmoid(tf.matmul(H0_outputs, ws1))\n",
    "hy = tf.sigmoid(tf.matmul(H1_outputs, ws2))\n",
    "#hy = tf.matmul(H0_outputs, ws1)\n",
    "\n",
    "#cost = tf.reduce_mean(tf.square(Y - hy))\n",
    "cost = tf.reduce_mean(tf.square(Y - hy) / 2)\n",
    "#cost = tf.reduce_mean(tf.nn.l2_loss(Y - hy))\n",
    "'''\n",
    "x_data_batch, y_data_batch = tf.train.batch(\n",
    "    [x_data, y_data],\n",
    "    batch_size = bs)\n",
    "'''\n",
    "x_data_batch = tf.train.batch([x_data], batch_size = bs)\n",
    "y_data_batch = tf.train.batch([y_data], batch_size = bs)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print (\"x_data_batch: \", x_data_batch)\n",
    "print (\"y_data_batch: \", y_data_batch)\n",
    "print (\"x_data_batch[0]: \", x_data_batch[0])\n",
    "print (\"tf.cast(x_data_batch[0][0][0], 'float'): \", tf.cast(x_data_batch[0][0][0], \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8143, 5)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "#print (x_data)\n",
    "print (x_data.shape)\n",
    "xx_data = x_data[0:100]\n",
    "print (xx_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Step: 0\n",
      "Cost: 0.129148\n",
      "test_accuracy:  0.210111\n",
      "   \n",
      "Step: 50\n",
      "Cost: 0.0719764\n",
      "test_accuracy:  0.705655\n",
      "   \n",
      "Step: 100\n",
      "Cost: 0.0715177\n",
      "test_accuracy:  0.705655\n",
      "   \n",
      "Step: 150\n",
      "Cost: 0.0726111\n",
      "test_accuracy:  0.705655\n",
      "   \n",
      "Step: 200\n",
      "Cost: 0.0842065\n",
      "test_accuracy:  0.705655\n",
      "   \n",
      "Step: 250\n",
      "Cost: 0.0993017\n",
      "test_accuracy:  0.705655\n",
      "   \n",
      "Step: 300\n",
      "Cost: 0.116723\n",
      "test_accuracy:  0.665099\n",
      "   \n",
      "Step: 350\n",
      "Cost: 0.124139\n",
      "test_accuracy:  0.641794\n",
      "   \n",
      "Step: 400\n",
      "Cost: 0.131237\n",
      "test_accuracy:  0.632995\n",
      "   \n",
      "Step: 450\n",
      "Cost: 0.134751\n",
      "test_accuracy:  0.631449\n",
      "   \n",
      "Step: 500\n",
      "Cost: 0.137011\n",
      "test_accuracy:  0.628417\n",
      "   \n",
      "Step: 550\n",
      "Cost: 0.140208\n",
      "test_accuracy:  0.625206\n",
      "   \n",
      "Step: 600\n",
      "Cost: 0.142046\n",
      "test_accuracy:  0.625028\n",
      "   \n",
      "Step: 650\n",
      "Cost: 0.143316\n",
      "test_accuracy:  0.626158\n",
      "   \n",
      "Step: 700\n",
      "Cost: 0.144197\n",
      "test_accuracy:  0.626039\n",
      "   \n",
      "Step: 750\n",
      "Cost: 0.144926\n",
      "test_accuracy:  0.626277\n",
      "   \n",
      "Step: 800\n",
      "Cost: 0.145421\n",
      "test_accuracy:  0.626693\n",
      "   \n",
      "Step: 850\n",
      "Cost: 0.146105\n",
      "test_accuracy:  0.631568\n",
      "   \n",
      "Step: 900\n",
      "Cost: 0.146723\n",
      "test_accuracy:  0.634124\n",
      "   \n",
      "Step: 950\n",
      "Cost: 0.147146\n",
      "test_accuracy:  0.635492\n",
      "test_accuracy:  0.638167\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: batch/fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/Const)]]\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "   \n",
    "    for step in xrange(epochs):\n",
    "        \n",
    "        #train network in batch size = bs\n",
    "        bs_i = 0\n",
    "        while bs_i <= data_size:\n",
    "            batch_end = 0\n",
    "            \n",
    "            if bs_i + 100 < data_size:\n",
    "                batch_end = bs_i + 100\n",
    "            else:\n",
    "                batch_end = data_size\n",
    "            xx_data_batch = x_data[bs_i : batch_end]\n",
    "            yy_data_batch = y_data[bs_i : batch_end]\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={X: xx_data_batch, Y: yy_data_batch})\n",
    "            bs_i += 100\n",
    "        \n",
    "        #sess.run(optimizer, feed_dict={X: xx_data_batch, Y: yy_data_batch})\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print (\"   \")\n",
    "            print (\"Step:\", step)\n",
    "            print (\"Cost:\", sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "            correct_prediction = tf.equal(tf.floor(hy + 0.5), Y)\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            test_accuracy = test_accuracy.eval({X: n_test_data, Y: o_test})\n",
    "            print (\"test_accuracy: \", test_accuracy)\n",
    "            \n",
    "    correct_prediction = tf.equal(tf.floor(hy + 0.5), Y)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    test_accuracy = test_accuracy.eval({X: n_test_data, Y: o_test})\n",
    "    print (\"test_accuracy: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
