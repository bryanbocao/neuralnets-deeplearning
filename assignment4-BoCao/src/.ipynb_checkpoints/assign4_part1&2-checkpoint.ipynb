{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nomQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109a82910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "N_TRAIN_IMAGE:  40000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)\n",
    "\n",
    "N_TRAIN_IMAGE = train_size\n",
    "print(\"N_TRAIN_IMAGE: \", N_TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "train_all_data_node = tf.constant(train_data)\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction = tf.nn.softmax(model(train_all_data_node))\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16982765e-01   1.42972870e-03   1.97592890e-05   3.99296312e-07\n",
      "   8.81408807e-04   1.68743473e-03   1.40850125e-05   8.78815353e-01\n",
      "   1.66529047e-04   2.52190534e-06]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(128, 10)\n",
      "All predictions [7 5 5 1 5 1 6 0 0 7 5 4 2 1 5 4 5 2 0 1 0 0 5 5 5 1 1 5 4 5 7 5 1 0 2 0 5\n",
      " 0 5 5 5 5 0 5 5 5 5 5 1 7 0 5 1 5 5 4 1 1 5 1 5 5 5 5 5 0 5 5 5 2 0 7 0 5\n",
      " 7 0 1 5 5 7 9 7 4 5 5 9 1 5 5 0 5 5 5 5 5 5 5 1 5 5 0 5 4 0 5 5 4 0 5 1 4\n",
      " 4 4 2 0 5 5 1 0 5 1 0 5 7 5 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n",
      "get_accuracy defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "    return error\n",
    "\n",
    "print('Error rate defined')\n",
    "\n",
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 40000\n",
      "Validation accuracy: 10.940000% (1094 of 10000), Mini-batch loss: 8.96989, Learning rate: 0.01000\n",
      "Step 2 of 40000\n",
      "Validation accuracy: 11.030000% (1103 of 10000), Mini-batch loss: 8.58483, Learning rate: 0.01000\n",
      "Step 4 of 40000\n",
      "Validation accuracy: 11.590000% (1159 of 10000), Mini-batch loss: 7.96280, Learning rate: 0.01000\n",
      "Step 6 of 40000\n"
     ]
    }
   ],
   "source": [
    "steps = train_size\n",
    "#steps = 500\n",
    "for step in range(steps):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "    _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "              validation_prediction.eval(), validation_labels)\n",
    "        print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "              (validation_accuracy * 100, validation_accuracy_fig, l, lr))\n",
    "        #feed_train_all_data_dict = {train_all_data_node: train_data}\n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "         #     train_all_data_prediction.eval(feed_dict=feed_train_all_data_dict), \n",
    "         #   train_labels)\n",
    "        \n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "        #      train_all_data_prediction.eval(), train_labels)\n",
    "        \n",
    "        #print('Train accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "         #     (train_accuracy * 100, train_accuracy_fig, l, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.8f (%s)' % (test_accuracy, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
