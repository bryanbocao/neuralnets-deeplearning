{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 10\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "N_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "N_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEH5X7OuzyRkIcAOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11838fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "validation_data: [[[[-0.36274511 -0.40196079 -0.39803922]\n",
      "   [-0.39411765 -0.41764706 -0.41764706]\n",
      "   [-0.40196079 -0.42156863 -0.4254902 ]\n",
      "   ..., \n",
      "   [ 0.01764706  0.00588235  0.00588235]\n",
      "   [ 0.02156863 -0.0254902  -0.04509804]\n",
      "   [-0.00588235 -0.04901961 -0.05686275]]\n",
      "\n",
      "  [[-0.25294119 -0.28431374 -0.30392158]\n",
      "   [-0.37450981 -0.39411765 -0.41764706]\n",
      "   [-0.43333334 -0.44901961 -0.46862745]\n",
      "   ..., \n",
      "   [-0.1        -0.12745099 -0.15490197]\n",
      "   [-0.0882353  -0.1509804  -0.20196079]\n",
      "   [-0.10784314 -0.1627451  -0.19803922]]\n",
      "\n",
      "  [[-0.11176471 -0.14313726 -0.17058824]\n",
      "   [-0.30784315 -0.32352942 -0.35490197]\n",
      "   [-0.44117647 -0.45294118 -0.48039216]\n",
      "   ..., \n",
      "   [-0.31960785 -0.33137256 -0.34705883]\n",
      "   [-0.29607844 -0.3392157  -0.36666667]\n",
      "   [-0.29607844 -0.32745099 -0.3392157 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.1509804   0.14705883  0.17058824]\n",
      "   [ 0.14313726  0.13921569  0.1509804 ]\n",
      "   [ 0.14313726  0.14313726  0.14705883]\n",
      "   ..., \n",
      "   [ 0.17843138  0.17450981  0.16666667]\n",
      "   [ 0.16666667  0.16666667  0.15882353]\n",
      "   [ 0.15490197  0.15490197  0.15490197]]\n",
      "\n",
      "  [[ 0.1627451   0.15882353  0.19019608]\n",
      "   [ 0.1627451   0.15882353  0.17843138]\n",
      "   [ 0.15882353  0.15882353  0.17058824]\n",
      "   ..., \n",
      "   [ 0.17450981  0.17058824  0.16666667]\n",
      "   [ 0.15882353  0.15490197  0.15490197]\n",
      "   [ 0.14705883  0.14705883  0.1509804 ]]\n",
      "\n",
      "  [[ 0.17843138  0.17450981  0.20196079]\n",
      "   [ 0.18627451  0.18235295  0.19803922]\n",
      "   [ 0.17843138  0.17843138  0.18627451]\n",
      "   ..., \n",
      "   [ 0.16666667  0.15882353  0.1627451 ]\n",
      "   [ 0.15882353  0.15490197  0.15882353]\n",
      "   [ 0.1509804   0.1509804   0.15882353]]]\n",
      "\n",
      "\n",
      " [[[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.43725491]\n",
      "   [-0.41764706 -0.43725491 -0.44509804]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  [[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.44117647]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  [[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.44509804]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.24117647 -0.28823531 -0.3392157 ]\n",
      "   [-0.18627451 -0.2372549  -0.29215688]\n",
      "   [-0.31568629 -0.36274511 -0.4254902 ]\n",
      "   ..., \n",
      "   [ 0.0254902   0.0254902  -0.10784314]\n",
      "   [-0.06862745 -0.05686275 -0.19803922]\n",
      "   [-0.11568628 -0.1        -0.24117647]]\n",
      "\n",
      "  [[-0.2647059  -0.31176472 -0.37058824]\n",
      "   [-0.28431374 -0.33137256 -0.39411765]\n",
      "   [-0.30392158 -0.3509804  -0.4137255 ]\n",
      "   ..., \n",
      "   [-0.01764706 -0.00980392 -0.17450981]\n",
      "   [-0.19019608 -0.18235295 -0.33529413]\n",
      "   [-0.21764706 -0.20980392 -0.3509804 ]]\n",
      "\n",
      "  [[-0.24901961 -0.28823531 -0.3509804 ]\n",
      "   [-0.28431374 -0.32352942 -0.38627452]\n",
      "   [-0.31176472 -0.3509804  -0.41764706]\n",
      "   ..., \n",
      "   [ 0.10784314  0.11568628 -0.06470589]\n",
      "   [ 0.03333334  0.0372549  -0.11960784]\n",
      "   [-0.15490197 -0.1509804  -0.30000001]]]\n",
      "\n",
      "\n",
      " [[[-0.04509804 -0.09607843 -0.28039217]\n",
      "   [-0.04901961 -0.0882353  -0.26862746]\n",
      "   [ 0.10784314  0.00196078 -0.17843138]\n",
      "   ..., \n",
      "   [ 0.18627451  0.01764706 -0.19803922]\n",
      "   [ 0.1627451   0.02156863 -0.21764706]\n",
      "   [ 0.05686275 -0.03333334 -0.29215688]]\n",
      "\n",
      "  [[-0.04509804 -0.1        -0.27254903]\n",
      "   [-0.02156863 -0.07647059 -0.24509804]\n",
      "   [ 0.1        -0.0254902  -0.19019608]\n",
      "   ..., \n",
      "   [ 0.0882353  -0.06470589 -0.2764706 ]\n",
      "   [ 0.06862745 -0.0254902  -0.2647059 ]\n",
      "   [ 0.02156863 -0.01764706 -0.28823531]]\n",
      "\n",
      "  [[-0.12745099 -0.1627451  -0.3392157 ]\n",
      "   [-0.11568628 -0.17058824 -0.32745099]\n",
      "   [ 0.05294118 -0.08431373 -0.2372549 ]\n",
      "   ..., \n",
      "   [ 0.06862745 -0.06078431 -0.27254903]\n",
      "   [-0.00588235 -0.06470589 -0.30000001]\n",
      "   [-0.00196078 -0.00980392 -0.25686276]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.19803922 -0.25294119 -0.38627452]\n",
      "   [-0.15490197 -0.21764706 -0.35490197]\n",
      "   [-0.2254902  -0.26862746 -0.39411765]\n",
      "   ..., \n",
      "   [-0.31176472 -0.34705883 -0.42156863]\n",
      "   [-0.04509804 -0.07647059 -0.17058824]\n",
      "   [ 0.12352941  0.05686275 -0.02156863]]\n",
      "\n",
      "  [[-0.28431374 -0.35490197 -0.43725491]\n",
      "   [-0.24509804 -0.31960785 -0.40588236]\n",
      "   [-0.23333333 -0.29215688 -0.38235295]\n",
      "   ..., \n",
      "   [-0.33529413 -0.38235295 -0.44901961]\n",
      "   [-0.00588235 -0.05294118 -0.14705883]\n",
      "   [ 0.12745099  0.07647059 -0.00980392]]\n",
      "\n",
      "  [[-0.19411765 -0.2764706  -0.35490197]\n",
      "   [-0.21764706 -0.30392158 -0.38235295]\n",
      "   [-0.2372549  -0.30392158 -0.37450981]\n",
      "   ..., \n",
      "   [-0.29607844 -0.35490197 -0.42941177]\n",
      "   [-0.01372549 -0.06862745 -0.17058824]\n",
      "   [ 0.10784314  0.06470589 -0.01372549]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[-0.00196078  0.12745099  0.10784314]\n",
      "   [ 0.04509804  0.17058824  0.17058824]\n",
      "   [ 0.10784314  0.24901961  0.22156863]\n",
      "   ..., \n",
      "   [-0.06862745  0.0882353   0.12745099]\n",
      "   [-0.02941176  0.11568628  0.14313726]\n",
      "   [-0.02941176  0.11568628  0.13921569]]\n",
      "\n",
      "  [[ 0.06078431  0.17058824  0.16666667]\n",
      "   [ 0.08431373  0.19411765  0.19411765]\n",
      "   [ 0.10784314  0.21764706  0.20196079]\n",
      "   ..., \n",
      "   [-0.01764706  0.15490197  0.17843138]\n",
      "   [ 0.0372549   0.17058824  0.19019608]\n",
      "   [ 0.06078431  0.19019608  0.20588236]]\n",
      "\n",
      "  [[ 0.07647059  0.1627451   0.17058824]\n",
      "   [ 0.05294118  0.13137256  0.13529412]\n",
      "   [ 0.05294118  0.12352941  0.11568628]\n",
      "   ..., \n",
      "   [-0.0254902   0.1627451   0.1627451 ]\n",
      "   [ 0.00588235  0.17058824  0.1627451 ]\n",
      "   [ 0.01372549  0.17450981  0.16666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.36274511  0.34705883  0.3509804 ]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   ..., \n",
      "   [ 0.32745099  0.29607844  0.30784315]\n",
      "   [ 0.30784315  0.29607844  0.30000001]\n",
      "   [ 0.30392158  0.28823531  0.29215688]]\n",
      "\n",
      "  [[ 0.36274511  0.34705883  0.3509804 ]\n",
      "   [ 0.35490197  0.3392157   0.34313726]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   ..., \n",
      "   [ 0.32352942  0.29215688  0.30392158]\n",
      "   [ 0.29215688  0.28039217  0.28431374]\n",
      "   [ 0.29607844  0.28039217  0.28431374]]\n",
      "\n",
      "  [[ 0.3509804   0.33529413  0.3392157 ]\n",
      "   [ 0.34705883  0.33137256  0.33529413]\n",
      "   [ 0.34705883  0.33137256  0.33529413]\n",
      "   ..., \n",
      "   [ 0.29215688  0.2647059   0.27254903]\n",
      "   [ 0.26078433  0.24901961  0.25294119]\n",
      "   [ 0.25686276  0.24509804  0.24901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.24509804  0.21764706  0.19019608]\n",
      "   [ 0.28431374  0.24509804  0.19411765]\n",
      "   [ 0.31568629  0.27254903  0.20588236]\n",
      "   ..., \n",
      "   [ 0.28431374  0.25294119  0.14705883]\n",
      "   [ 0.32352942  0.30392158  0.21764706]\n",
      "   [ 0.30784315  0.30000001  0.24901961]]\n",
      "\n",
      "  [[ 0.17843138  0.1509804   0.11176471]\n",
      "   [ 0.24509804  0.19803922  0.13137256]\n",
      "   [ 0.30784315  0.24901961  0.15882353]\n",
      "   ..., \n",
      "   [ 0.28823531  0.23333333  0.09607843]\n",
      "   [ 0.31960785  0.28039217  0.17450981]\n",
      "   [ 0.30392158  0.28039217  0.22156863]]\n",
      "\n",
      "  [[ 0.17843138  0.15882353  0.11960784]\n",
      "   [ 0.24901961  0.20980392  0.14313726]\n",
      "   [ 0.31568629  0.26078433  0.17450981]\n",
      "   ..., \n",
      "   [ 0.31568629  0.24901961  0.13137256]\n",
      "   [ 0.3392157   0.28431374  0.19411765]\n",
      "   [ 0.31176472  0.28039217  0.23333333]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.15490197  0.13921569  0.11176471]\n",
      "   [ 0.19411765  0.16666667  0.12352941]\n",
      "   [ 0.23333333  0.20196079  0.14705883]\n",
      "   ..., \n",
      "   [ 0.20980392  0.20588236  0.17058824]\n",
      "   [ 0.25294119  0.25294119  0.2254902 ]\n",
      "   [ 0.2647059   0.2647059   0.24901961]]\n",
      "\n",
      "  [[ 0.15490197  0.13921569  0.11568628]\n",
      "   [ 0.21372549  0.17843138  0.13529412]\n",
      "   [ 0.2647059   0.22156863  0.1627451 ]\n",
      "   ..., \n",
      "   [ 0.22941177  0.20588236  0.1627451 ]\n",
      "   [ 0.25686276  0.25294119  0.21764706]\n",
      "   [ 0.2647059   0.2647059   0.24901961]]\n",
      "\n",
      "  [[ 0.1627451   0.15490197  0.13529412]\n",
      "   [ 0.20588236  0.18235295  0.14705883]\n",
      "   [ 0.24509804  0.21764706  0.17450981]\n",
      "   ..., \n",
      "   [ 0.19803922  0.17843138  0.13921569]\n",
      "   [ 0.25294119  0.24509804  0.21372549]\n",
      "   [ 0.26862746  0.26862746  0.25294119]]]\n",
      "\n",
      "\n",
      " [[[ 0.19411765  0.12745099  0.02941176]\n",
      "   [ 0.18235295  0.12352941  0.0254902 ]\n",
      "   [ 0.21372549  0.14705883  0.04901961]\n",
      "   ..., \n",
      "   [ 0.29215688  0.2254902   0.12745099]\n",
      "   [ 0.28431374  0.20980392  0.12745099]\n",
      "   [ 0.28039217  0.19411765  0.11176471]]\n",
      "\n",
      "  [[ 0.19803922  0.13529412  0.0372549 ]\n",
      "   [ 0.19803922  0.13137256  0.0372549 ]\n",
      "   [ 0.20588236  0.14313726  0.04509804]\n",
      "   ..., \n",
      "   [ 0.32352942  0.23333333  0.13529412]\n",
      "   [ 0.34313726  0.22941177  0.1509804 ]\n",
      "   [ 0.35490197  0.2372549   0.15882353]]\n",
      "\n",
      "  [[ 0.2372549   0.18235295  0.08039216]\n",
      "   [ 0.24117647  0.17058824  0.07647059]\n",
      "   [ 0.22941177  0.1627451   0.06862745]\n",
      "   ..., \n",
      "   [ 0.37058824  0.25294119  0.17058824]\n",
      "   [ 0.39411765  0.25294119  0.17843138]\n",
      "   [ 0.38235295  0.2372549   0.1627451 ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.20196079 -0.30392158 -0.2764706 ]\n",
      "   [-0.22941177 -0.31960785 -0.30784315]\n",
      "   [-0.20196079 -0.28823531 -0.26078433]\n",
      "   ..., \n",
      "   [ 0.19019608  0.1509804   0.03333334]\n",
      "   [ 0.17843138  0.11960784  0.01372549]\n",
      "   [ 0.22156863  0.13921569  0.0372549 ]]\n",
      "\n",
      "  [[-0.02941176 -0.1509804  -0.13137256]\n",
      "   [-0.06862745 -0.17450981 -0.14705883]\n",
      "   [-0.08039216 -0.17843138 -0.13529412]\n",
      "   ..., \n",
      "   [ 0.1509804   0.09607843 -0.00588235]\n",
      "   [ 0.12745099  0.06470589 -0.02941176]\n",
      "   [ 0.14313726  0.07254902 -0.01764706]]\n",
      "\n",
      "  [[ 0.00588235 -0.1627451  -0.1627451 ]\n",
      "   [-0.00980392 -0.15490197 -0.12745099]\n",
      "   [-0.02156863 -0.14705883 -0.11568628]\n",
      "   ..., \n",
      "   [ 0.11960784  0.05294118 -0.03333334]\n",
      "   [ 0.14313726  0.07254902 -0.00196078]\n",
      "   [ 0.17058824  0.11176471  0.03333334]]]]\n",
      "validation_labels:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "print('validation_data:', validation_data)\n",
    "print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "(10, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16982765e-01   1.42972870e-03   1.97592890e-05   3.99296312e-07\n",
      "   8.81408807e-04   1.68743473e-03   1.40850125e-05   8.78815353e-01\n",
      "   1.66529047e-04   2.52190534e-06]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(10, 10)\n",
      "All predictions [7 5 5 1 5 1 6 0 0 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = np.zeros([10, 10], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Error rate defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 4000\n",
      "Mini-batch loss: 6.01374 Error: 50.00000 Learning rate: 0.01000\n",
      "Validation error: 89.8%\n",
      "Step 2 of 4000\n",
      "Mini-batch loss: 11.30708 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 89.1%\n",
      "Step 4 of 4000\n",
      "Mini-batch loss: 9.57967 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 87.6%\n",
      "Step 6 of 4000\n",
      "Mini-batch loss: 12.13719 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 87.5%\n",
      "Step 8 of 4000\n",
      "Mini-batch loss: 7.83263 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 89.1%\n",
      "Step 10 of 4000\n",
      "Mini-batch loss: 8.49578 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 86.7%\n",
      "Step 12 of 4000\n",
      "Mini-batch loss: 7.09521 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 87.1%\n",
      "Step 14 of 4000\n",
      "Mini-batch loss: 7.06248 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 84.8%\n",
      "Step 16 of 4000\n",
      "Mini-batch loss: 6.25485 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 84.7%\n",
      "Step 18 of 4000\n",
      "Mini-batch loss: 6.71318 Error: 70.00000 Learning rate: 0.01000\n",
      "Validation error: 85.6%\n",
      "Step 20 of 4000\n",
      "Mini-batch loss: 6.60025 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 86.9%\n",
      "Step 22 of 4000\n",
      "Mini-batch loss: 6.43348 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 89.2%\n",
      "Step 24 of 4000\n",
      "Mini-batch loss: 6.68706 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 89.0%\n",
      "Step 26 of 4000\n",
      "Mini-batch loss: 6.56696 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 88.6%\n",
      "Step 28 of 4000\n",
      "Mini-batch loss: 6.45118 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 86.8%\n",
      "Step 30 of 4000\n",
      "Mini-batch loss: 6.25868 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 85.0%\n",
      "Step 32 of 4000\n",
      "Mini-batch loss: 6.37228 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 84.4%\n",
      "Step 34 of 4000\n",
      "Mini-batch loss: 6.18018 Error: 70.00000 Learning rate: 0.01000\n",
      "Validation error: 84.2%\n",
      "Step 36 of 4000\n",
      "Mini-batch loss: 6.52512 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 84.6%\n",
      "Step 38 of 4000\n",
      "Mini-batch loss: 6.37233 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 85.0%\n",
      "Step 40 of 4000\n",
      "Mini-batch loss: 6.46988 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 86.5%\n",
      "Step 42 of 4000\n",
      "Mini-batch loss: 6.41323 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 87.7%\n",
      "Step 44 of 4000\n",
      "Mini-batch loss: 6.40737 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 88.2%\n",
      "Step 46 of 4000\n",
      "Mini-batch loss: 6.20234 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 87.6%\n",
      "Step 48 of 4000\n",
      "Mini-batch loss: 6.40750 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 86.6%\n",
      "Step 50 of 4000\n",
      "Mini-batch loss: 6.35410 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 85.5%\n",
      "Step 52 of 4000\n",
      "Mini-batch loss: 6.48062 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 85.1%\n",
      "Step 54 of 4000\n",
      "Mini-batch loss: 6.08256 Error: 60.00000 Learning rate: 0.01000\n",
      "Validation error: 85.3%\n",
      "Step 56 of 4000\n",
      "Mini-batch loss: 6.42064 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 84.8%\n",
      "Step 58 of 4000\n",
      "Mini-batch loss: 6.41094 Error: 100.00000 Learning rate: 0.01000\n",
      "Validation error: 84.3%\n",
      "Step 60 of 4000\n",
      "Mini-batch loss: 6.41183 Error: 90.00000 Learning rate: 0.01000\n",
      "Validation error: 83.7%\n",
      "Step 62 of 4000\n",
      "Mini-batch loss: 6.38880 Error: 80.00000 Learning rate: 0.01000\n",
      "Validation error: 83.4%\n",
      "Step 64 of 4000\n",
      "Mini-batch loss: 6.48942 Error: 90.00000 Learning rate: 0.01000\n"
     ]
    }
   ],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "steps = train_size // BATCH_SIZE\n",
    "for step in range(steps):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the graph and fetch some of the nodes.\n",
    "    _, l, lr, predictions = s.run(\n",
    "      [optimizer, loss, learning_rate, train_prediction],\n",
    "      feed_dict=feed_dict)\n",
    "    \n",
    "    # Print out the loss periodically.\n",
    "    if step % 2 == 0:\n",
    "        error, _ = error_rate(predictions, batch_labels)\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        print('Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "              validation_prediction.eval(), validation_labels)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
